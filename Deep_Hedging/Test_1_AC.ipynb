{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: utils\n",
      "  Building wheel for utils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13908 sha256=32e9ec29d3c83f9e7f790f4e0731f8fcef62564c0e2664f6a48f894eb55f41dd\n",
      "  Stored in directory: /Users/mcbookairdebat/Library/Caches/pip/wheels/4c/a5/a3/ab48e06c936b39960801612ee2767ff53764119f33d3d646e7\n",
      "Successfully built utils\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import scipy.stats as ss\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0,r,sigma,K,T,dt,num_paths,k=100.0, 0.01, 0.2,100.0, 1.0, 1/252, 10000,0\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_black_scholes_paths(S0=S0, r=r, sigma=sigma, T=T, dt=dt, num_paths=num_paths):\n",
    "    \"\"\"\n",
    "    Generate simulated stock price paths using Black-Scholes model.\n",
    "    \n",
    "    :param S0:       Initial stock price\n",
    "    :param r:        Risk-free rate\n",
    "    :param sigma:    Volatility\n",
    "    :param T:        Time horizon (in years)\n",
    "    :param dt:       Time step (in years)\n",
    "    :param num_paths: Number of simulated paths\n",
    "    :return:         Array of shape (num_paths, num_time_steps + 1)\n",
    "                     containing simulated stock paths\n",
    "    \"\"\"\n",
    "    # Number of time steps\n",
    "    num_steps = int(T / dt)\n",
    "    \n",
    "    # Pre-allocate the array for paths\n",
    "    # We'll store the entire evolution, so shape is (num_paths, num_steps + 1)\n",
    "    paths = np.zeros(( num_steps + 1))\n",
    "    \n",
    "    # Set initial prices\n",
    "    paths[0] = S0\n",
    "    \n",
    "    # Precompute constants for the Euler-Maruyama update\n",
    "    # Using log-Euler approach: S_{t+dt} = S_t * exp((r - 0.5 sigma^2) dt + sigma sqrt(dt) * Z)\n",
    "    drift = (r - 0.5 * sigma ** 2) * dt\n",
    "    diffusion = sigma * np.sqrt(dt)\n",
    "    \n",
    "    # Simulate paths\n",
    "    for t in range(num_steps):\n",
    "        # Draw random samples from the standard normal distribution\n",
    "        Z = np.random.normal(0, 1, num_paths)\n",
    "        # Update in log-space\n",
    "        paths[t+1] = paths[t] * np.exp(drift + diffusion * Z)\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m paths \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_black_scholes_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(paths\u001b[38;5;241m.\u001b[39mT)\n",
      "Cell \u001b[0;32mIn[160], line 34\u001b[0m, in \u001b[0;36mgenerate_black_scholes_paths\u001b[0;34m(S0, r, sigma, T, dt, num_paths)\u001b[0m\n\u001b[1;32m     32\u001b[0m     Z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, num_paths)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Update in log-space\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m paths[t] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(drift \u001b[38;5;241m+\u001b[39m diffusion \u001b[38;5;241m*\u001b[39m Z)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m paths\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "paths = generate_black_scholes_paths()\n",
    "plt.plot(paths.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_option_price(S, K, r, sigma, tau):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * tau) / (sigma * np.sqrt(tau))\n",
    "    d2 = d1 - sigma * np.sqrt(tau)\n",
    "    return S * norm.cdf(d1) - K * np.exp(-r * tau) * norm.cdf(d2)\n",
    "\n",
    "def call_option_delta(S, K, r, sigma, tau):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * tau) / (sigma * np.sqrt(tau))\n",
    "    return norm.cdf(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_prices = call_option_price(paths,K=K,r=r,sigma=sigma,tau=dt)\n",
    "option_delta = call_option_delta(paths,K=K,r=r,sigma=sigma,tau=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HedgingEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        S0=S0,\n",
    "        K=K,\n",
    "        r=r,\n",
    "        sigma=sigma,\n",
    "        T=T,\n",
    "        dt=dt,\n",
    "        kappa =k\n",
    "    ):\n",
    "        super(HedgingEnv, self).__init__()\n",
    "        #HedgingEnv.seed(1)\n",
    "        #HedgingEnv.action_space.seed(1)\n",
    "        # Store parameters\n",
    "        self.S0 = S0\n",
    "        self.K = K\n",
    "        self.r = r\n",
    "        self.sigma = sigma\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.num_steps = int(T / dt)\n",
    "        self.kappa = k\n",
    "\n",
    "        self.action_space = spaces.Discrete(20)\n",
    "        #self._action_spec = array_spec.BoundedArraySpec(\n",
    "            #shape=(), dtype=np.int32, minimum=0, maximum=19, name='action')\n",
    "        # Interpreted as how many units the agent can buy and sell \n",
    "        #self.action_space = spaces.Box(\n",
    "            #low=np.array([-1.0]), \n",
    "            #high=np.array([1.0]), \n",
    "            #dtype=np.float32\n",
    "        #)\n",
    "        self.action_values = np.linspace(-1.0,1,20)\n",
    "        \n",
    "        # Observation: (time_index, current_price,current_position)\n",
    "        # We'll keep it simple: there are all floats, so shape = (4,)\n",
    "        obs_low  = np.array([0.0,0.0,0.0], dtype=np.float32)\n",
    "        obs_high = np.array([self.num_steps, 1e9,1.0], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(obs_low, obs_high, dtype=np.float32)\n",
    "        \n",
    "        # Internal state\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # Generate a single path for this episode\n",
    "        self.path = generate_black_scholes_paths(S0=self.S0, r=self.r, sigma=self.sigma, T=self.T, dt=self.dt, num_paths=1)\n",
    "        # Reset time index\n",
    "        self.t = 0\n",
    "        # Current price\n",
    "        self.S = self.path[self.t]\n",
    "        # Current hedge position; initially zero\n",
    "        self.position = 0\n",
    "        # For simplicity, we track the cost of setting up the hedge\n",
    "        # We'll assume no transaction cost, but you can add it.\n",
    "        self.cash = 0\n",
    "        self.done = False\n",
    "        \n",
    "        return self._get_obs()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        new_position = action[0]\n",
    "        #3.1 Accouting P&L formulation \n",
    "        # Ri+1 = Vi+1 −Vi + Hi(Si+1 −Si) −κ|Si+1(Hi+1 −Hi)|\n",
    "\n",
    "        # Calculate cost to move from old position to new position\n",
    "        # For simplicity, we treat changes in position as immediate trades at price self.S\n",
    "        delta_position = new_position - self.position\n",
    "        cost = self.kappa* np.abs(delta_position * self.S)\n",
    "        \n",
    "        tau = (self.num_steps - self.t) * self.dt\n",
    "        call_px_i = call_option_price(self.S, self.K, self.r, self.sigma, tau)\n",
    "        stock = self.S\n",
    "        # Update our \"portfolio cash\" after trade\n",
    "        #self.cash -= cost\n",
    "        \n",
    "        # Update position\n",
    "        \n",
    "        \n",
    "        # Move to next time step\n",
    "        self.t += 1\n",
    "        self.S = self.path[self.t]\n",
    "\n",
    "        # Computing the different part of the reward \n",
    "        #Vi+1 −Vi\n",
    "        tau = (self.num_steps - self.t) * self.dt\n",
    "        call_px_i_1 = call_option_price(self.S, self.K, self.r, self.sigma, tau)\n",
    "        change_call_px = call_px_i_1 - call_px_i\n",
    "        # Hi(Si+1 −Si)\n",
    "        change_stock_value = self.position * (self.S - stock)\n",
    "        #κ|Si+1(Hi+1 −Hi)|\n",
    "        cost = self.kappa* np.abs(delta_position * self.S)\n",
    "\n",
    "        self.position = new_position\n",
    "        done = (self.t == self.num_steps)  # episode ends at maturity\n",
    "\n",
    "        #if not done:\n",
    "            #self.S = self.path[self.t]\n",
    "        \n",
    "        # Compute reward\n",
    "        #if done:\n",
    "            # At maturity, we have an option payoff = max(S-K, 0)\n",
    "            # Our portfolio is: position * S + cash\n",
    "            # Hedging error = (option payoff) - (portfolio value)\n",
    "            #option_payoff = max(self.S - self.K, 0)\n",
    "            #portfolio_value = self.position * self.S + self.cash  \n",
    "            #hedging_error = option_payoff - portfolio_value\n",
    "            \n",
    "            # Negative of squared error as final reward (for example)\n",
    "            #reward = - hedging_error\n",
    "        #else:\n",
    "            # No immediate reward until maturity in this simple version\n",
    "        \n",
    "        reward = change_call_px + change_stock_value - cost\n",
    "        \n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Return current time index and current price and current position holding \n",
    "\n",
    "        return np.array([self.t, self.S, self.position],dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Actor network: state -> action\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, action_dim=1, max_action=1.0):\n",
    "        super(Actor, self).__init__()\n",
    "        self.max_action = max_action\n",
    "        self.l1 = layers.Dense(128, activation='selu')\n",
    "        self.l2 = layers.Dense(128, activation='selu')\n",
    "        self.l3 = layers.Dense(20, activation='selu')\n",
    "        self.out = layers.Dense(action_dim,activation = 'tanh')  # no activation => raw output\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.l1(state)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        # scale output to [-max_action, max_action]\n",
    "        raw = self.out(x)\n",
    "        # For a single action dimension, we can do:\n",
    "        scaled = self.max_action * tf.tanh(raw)\n",
    "        return scaled  # shape=(batch_size, 1)\n",
    "\n",
    "# Critic network: state, action -> Q-value\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = layers.Dense(128, activation='selu')\n",
    "        self.l2 = layers.Dense(128, activation='selu')\n",
    "        self.l3 = layers.Dense(20,activation = 'selu')\n",
    "        self.out = layers.Dense(1)  # scalar Q-value\n",
    "\n",
    "    def call(self, state, action):\n",
    "        # Concatenate [state, action]\n",
    "        x = tf.concat([state, action], axis=-1)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, state_shape, action_dim=1):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "        self.states = np.zeros((capacity, state_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, state_shape), dtype=np.float32)\n",
    "        self.dones = np.zeros(capacity, dtype=np.float32)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        idx = self.position % self.capacity\n",
    "        \n",
    "        self.states[idx] = state\n",
    "        self.actions[idx] = action\n",
    "        self.rewards[idx] = reward\n",
    "        self.next_states[idx] = next_state\n",
    "        self.dones[idx] = float(done)\n",
    "        \n",
    "        self.position += 1\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(idx)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = random.sample(self.buffer, batch_size)\n",
    "        return (\n",
    "            self.states[indices],\n",
    "            self.actions[indices],\n",
    "            self.rewards[indices],\n",
    "            self.next_states[indices],\n",
    "            self.dones[indices]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_update(\n",
    "    actor, critic, \n",
    "    actor_optimizer, critic_optimizer,\n",
    "    replay_buffer, \n",
    "    batch_size=64, gamma=0.99\n",
    "):\n",
    "    # Sample batch\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states_tf = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "    actions_tf = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "    rewards_tf = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "    next_states_tf = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "    dones_tf = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "    \n",
    "    # -----------------------\n",
    "    # 1. Update Critic\n",
    "    # -----------------------\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Current Q(s,a)\n",
    "        Q_vals = critic(states_tf, actions_tf)\n",
    "\n",
    "        # Next a' = mu(next_state)\n",
    "        next_actions = actor(next_states_tf)\n",
    "\n",
    "        \n",
    "        # Q(s', a')\n",
    "        next_Q_vals = critic(next_states_tf, next_actions)\n",
    "        \n",
    "        \n",
    "        # Target y = r + gamma*(1-done)* Q(s', a')\n",
    "        y = tf.expand_dims(rewards_tf, -1) + gamma * (1.0 - tf.expand_dims(dones_tf, -1)) * next_Q_vals\n",
    "        \n",
    "        critic_loss = tf.reduce_mean((y - Q_vals)**2)\n",
    "        \n",
    "    \n",
    "    critic_grads = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "    critic_optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
    "    \n",
    "    # -----------------------\n",
    "    # 2. Update Actor\n",
    "    # -----------------------\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Actor tries to maximize Q(s, mu(s)) => minimize -Q()\n",
    "        current_actions = actor(states_tf)\n",
    "        \n",
    "        actor_loss = -tf.reduce_mean(critic(states_tf, current_actions))\n",
    "        \n",
    "    actor_grads = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "    actor_optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
    "    \n",
    "    return critic_loss, actor_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddpg(\n",
    "    env,\n",
    "    num_episodes=200,\n",
    "    max_steps_per_episode=9999,  # or env.num_steps\n",
    "    buffer_capacity=100000,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    lr_actor=1e-3,\n",
    "    lr_critic=1e-3\n",
    "):\n",
    "    # Create Actor & Critic\n",
    "    actor = Actor(action_dim=1, max_action=1)\n",
    "    critic = Critic()\n",
    "    \n",
    "    # Optimizers\n",
    "    actor_optimizer = tf.keras.optimizers.Adam(lr_actor)\n",
    "    critic_optimizer = tf.keras.optimizers.Adam(lr_critic)\n",
    "    \n",
    "    # Replay buffer\n",
    "    replay_buffer = ReplayBuffer(capacity=buffer_capacity, state_shape=3, action_dim=1)\n",
    "    \n",
    "    # For exploration, we might add noise to the actor's output\n",
    "    exploration_scale = 0.1\n",
    "    rewards = []\n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        \n",
    "        while not done and step_count < max_steps_per_episode:\n",
    "            step_count += 1\n",
    "            \n",
    "            # Actor forward pass\n",
    "            state_tf = tf.convert_to_tensor(state.reshape(1, -1), dtype=tf.float32)\n",
    "            action_tf = actor(state_tf)  # shape (1,1)\n",
    "            action = action_tf.numpy()[0]  # shape (1,)\n",
    "            \n",
    "            # Add noise for exploration\n",
    "            noise = np.random.normal(0, exploration_scale, size=(1,))\n",
    "            action_noisy = np.clip(action + noise, -1.0, 1.0)\n",
    "            \n",
    "            # Step environment\n",
    "            next_state, reward, done, _ = env.step(action_noisy)\n",
    "            \n",
    "            # Store transition\n",
    "            replay_buffer.push(state, action_noisy, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Train after enough samples\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                critic_loss, actor_loss = ddpg_update(\n",
    "                    actor, critic, \n",
    "                    actor_optimizer, critic_optimizer,\n",
    "                    replay_buffer,\n",
    "                    batch_size=batch_size,\n",
    "                    gamma=gamma\n",
    "                )\n",
    "        rewards.append(episode_reward)\n",
    "        print(f\"Episode {ep+1}/{num_episodes}: Reward = {episode_reward:.3f}\")\n",
    "    \n",
    "    return actor, critic, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/t3q2r39x05ndfqz7hm78wmkc0000gn/T/ipykernel_29731/152808958.py:34: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  paths[t+1] = paths[t] * np.exp(drift + diffusion * Z)\n",
      "/var/folders/lc/t3q2r39x05ndfqz7hm78wmkc0000gn/T/ipykernel_29731/3845289238.py:2: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * tau) / (sigma * np.sqrt(tau))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100: Reward = -16.549\n",
      "Episode 2/100: Reward = 19.995\n",
      "Episode 3/100: Reward = -1.841\n",
      "Episode 4/100: Reward = -5.437\n",
      "Episode 5/100: Reward = -8.854\n",
      "Episode 6/100: Reward = -1.698\n",
      "Episode 7/100: Reward = 20.113\n",
      "Episode 8/100: Reward = -7.910\n",
      "Episode 9/100: Reward = -5.685\n",
      "Episode 10/100: Reward = -5.851\n",
      "Episode 11/100: Reward = -4.540\n",
      "Episode 12/100: Reward = -8.462\n",
      "Episode 13/100: Reward = -8.030\n",
      "Episode 14/100: Reward = -3.979\n",
      "Episode 15/100: Reward = 0.879\n",
      "Episode 16/100: Reward = 12.811\n",
      "Episode 17/100: Reward = -1.081\n",
      "Episode 18/100: Reward = -5.971\n",
      "Episode 19/100: Reward = -3.940\n",
      "Episode 20/100: Reward = -5.026\n",
      "Episode 21/100: Reward = 9.330\n",
      "Episode 22/100: Reward = -0.656\n",
      "Episode 23/100: Reward = 9.587\n",
      "Episode 24/100: Reward = -7.834\n",
      "Episode 25/100: Reward = -3.945\n",
      "Episode 26/100: Reward = 7.246\n",
      "Episode 27/100: Reward = -9.282\n",
      "Episode 28/100: Reward = -2.683\n",
      "Episode 29/100: Reward = 4.871\n",
      "Episode 30/100: Reward = -5.188\n",
      "Episode 31/100: Reward = 10.799\n",
      "Episode 32/100: Reward = -7.405\n",
      "Episode 33/100: Reward = -11.969\n",
      "Episode 34/100: Reward = -7.798\n",
      "Episode 35/100: Reward = 9.979\n",
      "Episode 36/100: Reward = -4.793\n",
      "Episode 37/100: Reward = -8.239\n",
      "Episode 38/100: Reward = -6.208\n",
      "Episode 39/100: Reward = -9.014\n",
      "Episode 40/100: Reward = 3.759\n",
      "Episode 41/100: Reward = -9.284\n",
      "Episode 42/100: Reward = 18.929\n",
      "Episode 43/100: Reward = -6.433\n",
      "Episode 44/100: Reward = -8.736\n",
      "Episode 45/100: Reward = -10.077\n",
      "Episode 46/100: Reward = -6.335\n",
      "Episode 47/100: Reward = -11.022\n",
      "Episode 48/100: Reward = 0.069\n",
      "Episode 49/100: Reward = -6.951\n",
      "Episode 50/100: Reward = 6.042\n",
      "Episode 51/100: Reward = -8.425\n",
      "Episode 52/100: Reward = -5.623\n",
      "Episode 53/100: Reward = 0.763\n",
      "Episode 54/100: Reward = 4.575\n",
      "Episode 55/100: Reward = -7.110\n",
      "Episode 56/100: Reward = 8.308\n",
      "Episode 57/100: Reward = 6.824\n",
      "Episode 58/100: Reward = 5.016\n",
      "Episode 59/100: Reward = -2.640\n",
      "Episode 60/100: Reward = -6.623\n",
      "Episode 61/100: Reward = -8.207\n",
      "Episode 62/100: Reward = 10.803\n",
      "Episode 63/100: Reward = -5.145\n",
      "Episode 64/100: Reward = -6.181\n",
      "Episode 65/100: Reward = -2.626\n",
      "Episode 66/100: Reward = -1.862\n",
      "Episode 67/100: Reward = -8.095\n",
      "Episode 68/100: Reward = -5.558\n",
      "Episode 69/100: Reward = -2.060\n",
      "Episode 70/100: Reward = 8.514\n",
      "Episode 71/100: Reward = -9.089\n",
      "Episode 72/100: Reward = -1.728\n",
      "Episode 73/100: Reward = -3.738\n",
      "Episode 74/100: Reward = 1.626\n",
      "Episode 75/100: Reward = -10.258\n",
      "Episode 76/100: Reward = 4.191\n",
      "Episode 77/100: Reward = -1.581\n",
      "Episode 78/100: Reward = -6.070\n",
      "Episode 79/100: Reward = -4.400\n",
      "Episode 80/100: Reward = 7.352\n",
      "Episode 81/100: Reward = 10.099\n",
      "Episode 82/100: Reward = -7.751\n",
      "Episode 83/100: Reward = 3.192\n",
      "Episode 84/100: Reward = -7.148\n",
      "Episode 85/100: Reward = 1.027\n",
      "Episode 86/100: Reward = -6.174\n",
      "Episode 87/100: Reward = 0.327\n",
      "Episode 88/100: Reward = -1.848\n",
      "Episode 89/100: Reward = 3.542\n",
      "Episode 90/100: Reward = 15.867\n",
      "Episode 91/100: Reward = -0.111\n",
      "Episode 92/100: Reward = -1.096\n",
      "Episode 93/100: Reward = -1.807\n",
      "Episode 94/100: Reward = -0.653\n",
      "Episode 95/100: Reward = 6.470\n",
      "Episode 96/100: Reward = 2.322\n",
      "Episode 97/100: Reward = -7.389\n",
      "Episode 98/100: Reward = 3.951\n",
      "Episode 99/100: Reward = -9.746\n",
      "Episode 100/100: Reward = -11.428\n",
      "Test episode reward: -8.84237431515624\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = HedgingEnv(\n",
    "        S0=100.0,\n",
    "        K=100.0,\n",
    "        r=0.01,\n",
    "        sigma=0.2,\n",
    "        T=1.0,\n",
    "        dt=1/252,\n",
    "    )\n",
    "    \n",
    "    actor, critic,rewards = train_ddpg(env, num_episodes=100)\n",
    "    \n",
    "    # Test with the (greedy) actor, no noise\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    while not done:\n",
    "        state_tf = tf.convert_to_tensor(state.reshape(1, -1), dtype=tf.float32)\n",
    "        action = actor(state_tf).numpy()[0]\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Test episode reward:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  0.,  2.,  6., 16., 15., 11.,  4., 12.,  6.,  3.,  6.,  4.,\n",
       "         3.,  6.,  0.,  1.,  1.,  0.,  3.]),\n",
       " array([-16.54933998, -14.71619823, -12.88305649, -11.04991474,\n",
       "         -9.21677299,  -7.38363125,  -5.5504895 ,  -3.71734775,\n",
       "         -1.88420601,  -0.05106426,   1.78207749,   3.61521923,\n",
       "          5.44836098,   7.28150273,   9.11464447,  10.94778622,\n",
       "         12.78092797,  14.61406971,  16.44721146,  18.28035321,\n",
       "         20.11349495]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(rewards,bins =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.dense1 = layers.Dense(50, activation='selu')\n",
    "        self.dense2 = layers.Dense(50, activation='selu')\n",
    "        self.dense3 = layers.Dense(20, activation='selu')\n",
    "        self.out = layers.Dense(num_actions)  # No activation, raw Q-values\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished. Total reward: [-2976542.84133029]\n",
      "Final observation: [252 np.float64(138.65904705679307) np.float64(38.65904705679307)\n",
      " np.float64(1.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/t3q2r39x05ndfqz7hm78wmkc0000gn/T/ipykernel_29731/152808958.py:34: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  paths[t+1] = paths[t] * np.exp(drift + diffusion * Z)\n",
      "/var/folders/lc/t3q2r39x05ndfqz7hm78wmkc0000gn/T/ipykernel_29731/3845289238.py:2: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * tau) / (sigma * np.sqrt(tau))\n",
      "/var/folders/lc/t3q2r39x05ndfqz7hm78wmkc0000gn/T/ipykernel_29731/3845289238.py:7: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * tau) / (sigma * np.sqrt(tau))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = HedgingEnv()\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    while not done:\n",
    "        # Random action\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Episode finished. Total reward:\", total_reward)\n",
    "    print(\"Final observation:\", obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    \n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch = X[batch_idx]\n",
    "        yield X_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "retraining = False\n",
    "exponential_learning_rate_decay = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating checkpoints\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tflogs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## network architecture \n",
    "# Time series and network parameters\n",
    "n_steps = 52  # length of a time series\n",
    "n_inputs = 2  # dimensionality of input\n",
    "n_neurons = [50, 50, 20, 2]  # number of nodes in each layer\n",
    "activations = [tf.nn.selu, tf.nn.selu, tf.nn.selu, None]  # activation functions\n",
    "n_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up performance scheduling\n",
    "max_checks_without_progress = 10\n",
    "checks_without_progress = 0\n",
    "if not retraining:\n",
    "    best_loss = np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) decay parameters for exponential decay\n",
    "# decay_steps = 4000  \n",
    "# decay_rate = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "initial_learning_rate = 0.00005\n",
    "n_epochs = 400\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model using the Keras Functional API\n",
    "inputs = tf.keras.Input(shape=(n_steps, n_inputs))\n",
    "# Example RNN layers\n",
    "layer = tf.keras.layers.SimpleRNN(n_neurons[0], activations[0], return_sequences=True)(inputs)\n",
    "layer = tf.keras.layers.SimpleRNN(n_neurons[1], activations[1], return_sequences=True)(layer)\n",
    "layer = tf.keras.layers.SimpleRNN(n_neurons[2], activations[2], return_sequences=True)(layer)\n",
    "outputs = tf.keras.layers.SimpleRNN(n_neurons[3], activations[3], return_sequences=False)(layer)\n",
    "\n",
    "# Create a Keras model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up performance scheduling\n",
    "max_checks_without_progress = 10\n",
    "checks_without_progress = 0\n",
    "if not retraining:\n",
    "    best_loss = np.inf\n",
    "\n",
    "# (Optional) decay parameters for exponential decay\n",
    "# decay_steps = 4000  \n",
    "# decay_rate = 1/3\n",
    "\n",
    "# Training parameters\n",
    "initial_learning_rate = 0.00005\n",
    "n_epochs = 400\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(100.0, 0.5),             \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_4 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │         \u001b[38;5;34m2,650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_5 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │         \u001b[38;5;34m5,050\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_6 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m20\u001b[0m)         │         \u001b[38;5;34m1,420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_7 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m46\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,166</span> (35.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,166\u001b[0m (35.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,166</span> (35.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,166\u001b[0m (35.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables initializer and Saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # 1) Training step\n",
    "    for X_batch, Y_batch in shuffle_batch(X_train, Y_train, batch_size):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X_batch, training=True)\n",
    "            batch_loss = loss_fn(Y_batch, predictions)\n",
    "        grads = tape.gradient(batch_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # 2) Compute losses for entire training & validation sets\n",
    "    #    (In practice, you might do this in mini-batches if large.)\n",
    "    train_predictions = model(X_train, training=False)\n",
    "    loss_train = loss_fn(Y_train, train_predictions).numpy()\n",
    "\n",
    "    val_predictions = model(X_validation, training=False)\n",
    "    loss_val = loss_fn(Y_validation, val_predictions).numpy()\n",
    "\n",
    "    # 3) Performance scheduling (similar logic to your old code)\n",
    "    if not exponential_learning_rate_decay:\n",
    "        # If current validation is better than the best so far\n",
    "        if loss_val < best_loss:\n",
    "            # Save the model\n",
    "            model.save(\"./my_dissertation_model.h5\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if checks_without_progress > max_checks_without_progress:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    # 4) Print progress\n",
    "    # If you had an LR schedule, you'd do something like:\n",
    "    # current_lr = optimizer.learning_rate(current_step).numpy() if callable(optimizer.learning_rate) else optimizer.learning_rate\n",
    "    # For a static LR:\n",
    "    current_lr = optimizer.learning_rate.numpy() if hasattr(optimizer.learning_rate, 'numpy') else optimizer.learning_rate\n",
    "\n",
    "    if exponential_learning_rate_decay:\n",
    "        # mimic your old print with .eval() — in TF2, we can .numpy()\n",
    "        print(f\"{epoch}\\tCurrent train loss: {loss_train:.6f}\\tCurrent val loss: {loss_val:.6f}\\t\"\n",
    "              f\"Best loss: {best_loss:.6f}\\tLearning rate: {current_lr:.6f}\")\n",
    "    else:\n",
    "        print(f\"{epoch}\\tCurrent train loss: {loss_train:.6f}\\tCurrent val loss: {loss_val:.6f}\\t\"\n",
    "              f\"Best loss: {best_loss:.6f}\\tLearning rate: {current_lr:.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving final results\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my dissertation model.ckpt\")\n",
    "    print(\"Final loss: {:.6f}\".format(best_loss))\n",
    "\n",
    "    outputs_validation = sess.run(outputs, feed_dict={X: X_validation})\n",
    "    outputs_train = sess.run(outputs, feed_dict={X: X_train})\n",
    "    outputs_test = sess.run(outputs, feed_dict={X: X_test})\n",
    "\n",
    "    loss_validation = sess.run(loss, feed_dict={X: X_validation})\n",
    "    loss_train = sess.run(loss, feed_dict={X: X_train})\n",
    "    loss_test = sess.run(loss, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RNN</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,166</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn (\u001b[38;5;33mRNN\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │         \u001b[38;5;34m9,166\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,166</span> (35.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,166\u001b[0m (35.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,166</span> (35.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,166\u001b[0m (35.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up and running with TensorFlow 2.x!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# 1) Hyperparameters\n",
    "# -------------------------\n",
    "n_steps = 52   # length of a time series\n",
    "n_inputs = 2   # dimensionality of input\n",
    "n_neurons = [50, 50, 20, 2]  # number of nodes in each layer\n",
    "activations = [tf.nn.selu, tf.nn.selu, tf.nn.selu, None]  # activation functions\n",
    "n_layers = 4\n",
    "\n",
    "# Performance scheduling\n",
    "max_checks_without_progress = 10\n",
    "checks_without_progress = 0\n",
    "retraining = False  # Suppose we define this somewhere\n",
    "best_loss = np.inf\n",
    "\n",
    "# (Optional) Exponential decay parameters\n",
    "exponential_learning_rate_decay = False  # toggle it as needed\n",
    "decay_steps = 4000\n",
    "decay_rate = 1/3\n",
    "\n",
    "# Training parameters\n",
    "initial_learning_rate = 0.00005\n",
    "n_epochs = 400\n",
    "batch_size = 200\n",
    "\n",
    "# -------------------------\n",
    "# 2) Define a custom entropic loss (placeholder implementation)\n",
    "#    Replace with your actual entropic logic if you have it.\n",
    "# -------------------------\n",
    "def entropic_loss(alpha=100.0, beta=0.5):\n",
    "    \"\"\"\n",
    "    A placeholder for the custom entropic loss from your original code.\n",
    "    Modify this function as per your real formula.\n",
    "    \"\"\"\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Example dummy formula for demonstration:\n",
    "        #  (Not actually \"entropic\", just for structure)\n",
    "        return alpha * tf.reduce_mean(tf.abs(y_true - y_pred)) + beta\n",
    "    return loss_fn\n",
    "\n",
    "# -------------------------\n",
    "# 3) Build the Model using tf.keras\n",
    "# -------------------------\n",
    "# Instead of placeholders, we define an Input layer describing the shape:\n",
    "inputs = tf.keras.Input(shape=(n_steps, n_inputs))\n",
    "\n",
    "# Create multiple RNN cells with the specified activations:\n",
    "# If you strictly want \"BasicRNNCell\" behavior from TF1.x, you can use \"SimpleRNNCell\" in TF2.\n",
    "rnn_cells = []\n",
    "for i in range(n_layers):\n",
    "    units = n_neurons[i]\n",
    "    activation = activations[i]\n",
    "    # We'll use SimpleRNNCell to mimic BasicRNNCell\n",
    "    rnn_cells.append(tf.keras.layers.SimpleRNNCell(units=units, activation=activation))\n",
    "\n",
    "# Stack them in a single RNN layer:\n",
    "# return_sequences=True to pass output from each time step,\n",
    "# return_state=False unless you specifically need final states.\n",
    "rnn_layer = tf.keras.layers.RNN(rnn_cells, return_sequences=True, return_state=False)\n",
    "\n",
    "# Get the outputs\n",
    "outputs = rnn_layer(inputs)\n",
    "\n",
    "# Create a Keras Model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Set up the (optional) learning-rate schedule\n",
    "# -------------------------\n",
    "if exponential_learning_rate_decay:\n",
    "    # If you want actual dynamic decay, use a schedules.ExponentialDecay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=False  # or True depending on your preference\n",
    "    )\n",
    "    current_lr = lr_schedule\n",
    "else:\n",
    "    current_lr = initial_learning_rate\n",
    "\n",
    "# -------------------------\n",
    "# 5) Compile the model with Adam and the custom entropic loss\n",
    "# -------------------------\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=current_lr),\n",
    "    loss=entropic_loss(100.0, 0.5)\n",
    ")\n",
    "\n",
    "# Just a quick check\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# 6) Example dummy training loop (if you have data)\n",
    "# -------------------------\n",
    "# Suppose X_train is shape (num_samples, n_steps, n_inputs)\n",
    "# and y_train is shape (num_samples, n_steps, 2) or something that matches outputs:\n",
    "# \n",
    "# X_train = np.random.randn(1000, n_steps, n_inputs).astype(np.float32)\n",
    "# y_train = np.random.randn(1000, n_steps, 2).astype(np.float32)\n",
    "# \n",
    "# model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size)\n",
    "\n",
    "print(\"Up and running with TensorFlow 2.x!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
