{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/mcbookairdebat/Desktop/Projet/project/GitHub/Deep_hedging_research/Deep_Hedging/Deep_hedging_Hull/')\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import get_sim_path,compute_rewards\n",
    "#from envs import  TradingEnv\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Lambda,\n",
    "    concatenate,\n",
    "    BatchNormalization\n",
    ")\n",
    "\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    trading environment;\n",
    "    \"\"\"\n",
    "\n",
    "    # trade_freq in unit of day, e.g 2: every 2 day; 0.5 twice a day;\n",
    "    def __init__(self,num_sim=500002,\n",
    "        continuous_action_flag=True, spread=0, init_ttm=20, trade_freq=1, num_contract=1,check_for_pretrained=False):\n",
    "\n",
    "        # simulated data: array of asset price, option price and delta paths (num_path x num_period)\n",
    "        # generate data now\n",
    "        \n",
    "        self.path, self.option_price_path, self.delta_path = get_sim_path(M=init_ttm, freq=trade_freq, num_sim=num_sim)\n",
    "        # other attributes\n",
    "        self.num_path = self.path.shape[0]\n",
    "\n",
    "        # set num_period: initial time to maturity * daily trading freq + 1 (see get_sim_path() in utils.py)\n",
    "        self.num_period = self.path.shape[1]\n",
    "        # print(\"***\", self.num_period)\n",
    "\n",
    "        # time to maturity array\n",
    "        self.ttm_array = np.arange(init_ttm, -trade_freq, -trade_freq)\n",
    "\n",
    "\n",
    "        # spread\n",
    "        self.spread = spread\n",
    "        self.check_for_pretrained = check_for_pretrained\n",
    "        # step function initialization depending on cash_flow_flag\n",
    "        \n",
    "        self.step = self.step_profit_loss\n",
    "\n",
    "        self.num_contract = num_contract\n",
    "        self.strike_price = 100\n",
    "\n",
    "        # track the index of simulated path in use\n",
    "        self.sim_episode = -1\n",
    "\n",
    "        # track time step within an episode (it's step)\n",
    "        self.t = None\n",
    "\n",
    "        # action space\n",
    "        if continuous_action_flag:\n",
    "            self.action_space = spaces.Box(low=np.array([0]), high=np.array([num_contract]), dtype=np.float32)\n",
    "        else:\n",
    "            self.num_action = num_contract * 1 + 1\n",
    "            self.action_space = spaces.Discrete(self.num_action)\n",
    "\n",
    "        self.num_state = 3\n",
    "\n",
    "        self.state = []\n",
    "\n",
    "      \n",
    "\n",
    "   \n",
    "\n",
    "    def reset(self):\n",
    "        # repeatedly go through available simulated paths (if needed)\n",
    "        self.sim_episode = (self.sim_episode + 1) % self.num_path\n",
    "        self.t = 0\n",
    "\n",
    "        price = self.path[self.sim_episode, self.t]\n",
    "        position = self.delta_path[self.sim_episode,self.t]\n",
    "\n",
    "        ttm = self.ttm_array[self.t]\n",
    "\n",
    "        self.state = [price, position, ttm]\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def step_profit_loss(self, action):\n",
    "        \"\"\"\n",
    "        profit loss period reward\n",
    "        \"\"\"\n",
    "\n",
    "        # current prices (at t)\n",
    "        current_price = self.state[0]\n",
    "        current_option_price = self.option_price_path[self.sim_episode, self.t]\n",
    "\n",
    "        # current position\n",
    "        current_position = self.state[1]\n",
    "\n",
    "        # update time\n",
    "        self.t = self.t + 1\n",
    "        print(self.t)\n",
    "        # get state for tomorrow (at t + 1)\n",
    "        price = self.path[self.sim_episode, self.t]\n",
    "        option_price = self.option_price_path[self.sim_episode, self.t]\n",
    "        position = action\n",
    "        \n",
    "        ttm = self.ttm_array[self.t]\n",
    "        #print(price, position, ttm)\n",
    "        self.state = [price, position, ttm]\n",
    "        #print('state',self.state)\n",
    "\n",
    "        # calculate period reward (part 1)\n",
    "        reward = (price - current_price) * position/100 - np.abs(current_position - position) * current_price * self.spread/100\n",
    "\n",
    "        # if tomorrow is end of episode\n",
    "        if self.t == self.num_period-1:\n",
    "            done = True\n",
    "            reward = reward - (max(price - self.strike_price, 0) - current_option_price) * self.num_contract * 1 - position * price * self.spread/100\n",
    "        else:\n",
    "            done = False\n",
    "            reward = reward - (option_price - current_option_price) * self.num_contract * 1\n",
    "\n",
    "        # for other info later\n",
    "        info = {\"path_row\": self.sim_episode}\n",
    "\n",
    "        return self.state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [np.random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, size, alpha):\n",
    "        \"\"\"Create Prioritized Replay buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        alpha: float\n",
    "            how much prioritization is used\n",
    "            (0 - no prioritization, 1 - full prioritization)\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        ReplayBuffer.__init__\n",
    "        \"\"\"\n",
    "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
    "        assert alpha >= 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "    def add(self, *args, **kwargs):\n",
    "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
    "        idx = self._next_idx\n",
    "        super().add(*args, **kwargs)\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        p_total = self._it_sum.sum(0, len(self._storage) - 1)\n",
    "        every_range_len = p_total / batch_size\n",
    "        for i in range(batch_size):\n",
    "            mass = np.random.random() * every_range_len + i * every_range_len\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "\n",
    "        compared to ReplayBuffer.sample\n",
    "        it also returns importance weights and idxes\n",
    "        of sampled experiences.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        beta: float\n",
    "            To what degree to use importance weights\n",
    "            (0 - no corrections, 1 - full correction)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        weights: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "            denoting importance weight of each sampled transition\n",
    "        idxes: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "            idexes in buffer of sampled experiences\n",
    "        \"\"\"\n",
    "        assert beta > 0\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = np.array(weights)\n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return tuple(list(encoded_sample) + [weights, idxes])\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions.\n",
    "\n",
    "        sets priority of transition at index idxes[i] in buffer\n",
    "        to priorities[i].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idxes: [int]\n",
    "            List of idxes of sampled transitions\n",
    "        priorities: [float]\n",
    "            List of updated priorities corresponding to\n",
    "            transitions at the sampled idxes denoted by\n",
    "            variable `idxes`.\n",
    "        \"\"\"\n",
    "        assert len(idxes) == len(priorities)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self._storage)\n",
    "            self._it_sum[idx] = priority ** self._alpha\n",
    "            self._it_min[idx] = priority ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, priority)\n",
    "\n",
    "class Schedule(object):\n",
    "    def value(self, t):\n",
    "        \"\"\"Value of the schedule at time t\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ConstantSchedule(object):\n",
    "    def __init__(self, value):\n",
    "        \"\"\"Value remains constant over time.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value: float\n",
    "            Constant value of the schedule\n",
    "        \"\"\"\n",
    "        self._v = value\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        return self._v\n",
    "\n",
    "\n",
    "def linear_interpolation(l, r, alpha):\n",
    "    return l + alpha * (r - l)\n",
    "\n",
    "\n",
    "class PiecewiseSchedule(object):\n",
    "    def __init__(self, endpoints, interpolation=linear_interpolation, outside_value=None):\n",
    "        \"\"\"Piecewise schedule.\n",
    "\n",
    "        endpoints: [(int, int)]\n",
    "            list of pairs `(time, value)` meanining that schedule should output\n",
    "            `value` when `t==time`. All the values for time must be sorted in\n",
    "            an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n",
    "            and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n",
    "            `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n",
    "            time passed between `time_a` and `time_b` for time `t`.\n",
    "        interpolation: lambda float, float, float: float\n",
    "            a function that takes value to the left and to the right of t according\n",
    "            to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n",
    "            right endpoint that t has covered. See linear_interpolation for example.\n",
    "        outside_value: float\n",
    "            if the value is requested outside of all the intervals sepecified in\n",
    "            `endpoints` this value is returned. If None then AssertionError is\n",
    "            raised when outside value is requested.\n",
    "        \"\"\"\n",
    "        idxes = [e[0] for e in endpoints]\n",
    "        assert idxes == sorted(idxes)\n",
    "        self._interpolation = interpolation\n",
    "        self._outside_value = outside_value\n",
    "        self._endpoints = endpoints\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        for (l_t, l), (r_t, r) in zip(self._endpoints[:-1], self._endpoints[1:]):\n",
    "            if l_t <= t and t < r_t:\n",
    "                alpha = float(t - l_t) / (r_t - l_t)\n",
    "                return self._interpolation(l, r, alpha)\n",
    "\n",
    "        # t does not belong to any of the pieces, so doom.\n",
    "        assert self._outside_value is not None\n",
    "        return self._outside_value\n",
    "\n",
    "\n",
    "class LinearSchedule(object):\n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
    "        \"\"\"Linear interpolation between initial_p and final_p over\n",
    "        schedule_timesteps. After this many timesteps pass final_p is\n",
    "        returned.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        schedule_timesteps: int\n",
    "            Number of timesteps for which to linearly anneal initial_p\n",
    "            to final_p\n",
    "        initial_p: float\n",
    "            initial output value\n",
    "        final_p: float\n",
    "            final output value\n",
    "        \"\"\"\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.final_p = final_p\n",
    "        self.initial_p = initial_p\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n",
    "        return self.initial_p + fraction * (self.final_p - self.initial_p)\n",
    "\n",
    "class SegmentTree(object):\n",
    "    def __init__(self, capacity, operation, neutral_element):\n",
    "        \"\"\"Build a Segment Tree data structure.\n",
    "\n",
    "        https://en.wikipedia.org/wiki/Segment_tree\n",
    "\n",
    "        Can be used as regular array, but with two\n",
    "        important differences:\n",
    "\n",
    "            a) setting item's value is slightly slower.\n",
    "               It is O(lg capacity) instead of O(1).\n",
    "            b) user has access to an efficient ( O(log segment size) )\n",
    "               `reduce` operation which reduces `operation` over\n",
    "               a contiguous subsequence of items in the array.\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        capacity: int\n",
    "            Total size of the array - must be a power of two.\n",
    "        operation: lambda obj, obj -> obj\n",
    "            and operation for combining elements (eg. sum, max)\n",
    "            must form a mathematical group together with the set of\n",
    "            possible values for array elements (i.e. be associative)\n",
    "        neutral_element: obj\n",
    "            neutral element for the operation above. eg. float('-inf')\n",
    "            for max and 0 for sum.\n",
    "        \"\"\"\n",
    "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n",
    "        self._capacity = capacity\n",
    "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
    "        self._operation = operation\n",
    "\n",
    "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
    "        if start == node_start and end == node_end:\n",
    "            return self._value[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self._operation(\n",
    "                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
    "                )\n",
    "\n",
    "    def reduce(self, start=0, end=None):\n",
    "        \"\"\"Returns result of applying `self.operation`\n",
    "        to a contiguous subsequence of the array.\n",
    "\n",
    "            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        start: int\n",
    "            beginning of the subsequence\n",
    "        end: int\n",
    "            end of the subsequences\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reduced: obj\n",
    "            result of reducing self.operation over the specified range of array elements.\n",
    "        \"\"\"\n",
    "        if end is None:\n",
    "            end = self._capacity\n",
    "        if end < 0:\n",
    "            end += self._capacity\n",
    "        end -= 1\n",
    "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        # index of the leaf\n",
    "        idx += self._capacity\n",
    "        self._value[idx] = val\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self._value[idx] = self._operation(\n",
    "                self._value[2 * idx],\n",
    "                self._value[2 * idx + 1]\n",
    "            )\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self._capacity\n",
    "        return self._value[self._capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=operator.add,\n",
    "            neutral_element=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start=0, end=None):\n",
    "        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n",
    "        return super(SumSegmentTree, self).reduce(start, end)\n",
    "\n",
    "    def find_prefixsum_idx(self, prefixsum):\n",
    "        \"\"\"Find the highest index `i` in the array such that\n",
    "            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n",
    "\n",
    "        if array values are probabilities, this function\n",
    "        allows to sample indexes according to the discrete\n",
    "        probability efficiently.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfixsum: float\n",
    "            upperbound on the sum of array prefix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            highest index satisfying the prefixsum constraint\n",
    "        \"\"\"\n",
    "        assert 0 <= prefixsum <= self.sum() + 1e-5\n",
    "        idx = 1\n",
    "        while idx < self._capacity:  # while non-leaf\n",
    "            if self._value[2 * idx] > prefixsum:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                prefixsum -= self._value[2 * idx]\n",
    "                idx = 2 * idx + 1\n",
    "        return idx - self._capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=min,\n",
    "            neutral_element=float('inf')\n",
    "        )\n",
    "\n",
    "    def min(self, start=0, end=None):\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n",
    "\n",
    "        return super(MinSegmentTree, self).reduce(start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRL:\n",
    "    def __init__(self):\n",
    "        if not os.path.exists('model'):\n",
    "            os.mkdir('model')\n",
    "\n",
    "        if not os.path.exists('history'):\n",
    "            os.mkdir('history')\n",
    "\n",
    "    def test(self, total_episode, delta_flag=False):\n",
    "        \"\"\"hedge with model.\n",
    "        \"\"\"\n",
    "        print('testing...')\n",
    "\n",
    "        self.epsilon = -1\n",
    "        w_T_store = []\n",
    "        actions =[]\n",
    "        Y_0 =[]\n",
    "        rewards =[]\n",
    "        deltas, stock_prices, option_prices,w_T_means,w_T_vars  = [],[],[],[],[]\n",
    "        for i in range(total_episode):\n",
    "            action_store = []\n",
    "            observation = self.env.reset()\n",
    "            done = False\n",
    "            reward_store = []\n",
    "            while not done:\n",
    "                # prepare state\n",
    "                try:\n",
    "                    x = np.array(observation).reshape(1, -1)\n",
    "                except:\n",
    "                    observation = [observation[0],observation[1][0], observation[2]]\n",
    "                    x = np.array(observation).reshape(1, -1)\n",
    "\n",
    "                if delta_flag:\n",
    "                    action= self.env.delta_path[i % self.env.num_path, self.env.t] * self.env.num_contract * 1\n",
    "                else:\n",
    "                    # choose action from epsilon-greedy; epsilon has been set to -1\n",
    "                    action, _, _ = self.egreedy_action(x)\n",
    "                    action[0] =action[0]/100\n",
    "                # store action to take a look\n",
    "\n",
    "                try:\n",
    "                    action_store.append(action[0])\n",
    "                except:\n",
    "                    action_store.append(action)\n",
    "                observation, reward, done, info = self.env.step(action)\n",
    "\n",
    "                try:\n",
    "                    reward_store.extend(reward)\n",
    "                except:\n",
    "                    reward_store.append(reward)\n",
    "\n",
    "\n",
    "\n",
    "            # get final wealth at the end of episode, and store it.\n",
    "            w_T = sum(reward_store)\n",
    "            w_T_store.append(w_T)\n",
    "            actions.append(action_store)\n",
    "            \n",
    "            w_T_mean = np.mean(w_T_store)\n",
    "            w_T_var = np.var(w_T_store)\n",
    "            path_row = info[\"path_row\"]\n",
    "            #print(path_row)\n",
    "            #print(info)\n",
    "            '''with np.printoptions(precision=2, suppress=True):\n",
    "                #print(\"episode: {} | final wealth: {:.2f}; so far mean and variance of final wealth was {} and {}\".format(i, w_T, w_T_mean, w_T_var))\n",
    "                print(\"episode: {} | so far Y(0): {:.2f}\".format(i, -w_T_mean + self.ra_c * np.sqrt(w_T_var)))\n",
    "                print(\"episode: {} | rewards: {}\".format(i, np.array(reward_store)))\n",
    "                print(\"episode: {} | action taken: {}\".format(i, np.array(action_store)/100))\n",
    "                print(\"episode: {} | deltas {}\".format(i, self.env.delta_path[path_row]))\n",
    "                print(\"episode: {} | stock price {}\".format(i, self.env.path[path_row]))\n",
    "                print(\"episode: {} | option price {}\\n\".format(i, self.env.option_price_path[path_row]))'''\n",
    "            \n",
    "            Y_0.append(-w_T_mean + self.ra_c * np.sqrt(w_T_var))\n",
    "            rewards.append(reward_store)\n",
    "            deltas.append(self.env.delta_path[path_row])\n",
    "            stock_prices.append(self.env.path[path_row])\n",
    "            option_prices.append(self.env.option_price_path[path_row])\n",
    "            w_T_means.append(w_T_mean)\n",
    "            w_T_vars.append(np.sqrt(w_T_var))\n",
    "\n",
    "        #return {key: value for key, value in locals().items()}\n",
    "        return w_T_store,Y_0,rewards,actions,deltas,stock_prices,option_prices,w_T_means,w_T_vars\n",
    "\n",
    "    def plot(self, history):\n",
    "        pass\n",
    "\n",
    "    def save_history(self, history, name):\n",
    "        name = os.path.join('history', name)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(history)\n",
    "        df.to_csv(name, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(DRL):  # or DDPG(DRL) if you inherit from DRL\n",
    "    \"\"\"\n",
    "    Deep Deterministic Policy Gradient - TF2 version\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(DDPG, self).__init__()  # if you have a parent class DRL\n",
    "\n",
    "        self.env = env\n",
    "        self.upper_bound = self.env.action_space.high[0]\n",
    "        self.lower_bound = self.env.action_space.low[0]\n",
    "\n",
    "        # update rate for target model.\n",
    "        self.TAU = 0.0001\n",
    "\n",
    "        # learning rates\n",
    "        self.actor_lr = 1e-4\n",
    "        self.critic_lr = 1e-4\n",
    "\n",
    "        # risk aversion constant\n",
    "        self.ra_c = 1.5\n",
    "        # Actor and Critic network\n",
    "\n",
    "        self.actor = self._build_actor(learning_rate=self.actor_lr)\n",
    "        self.critic_Q_ex, self.critic_Q_ex2, self.critic_Q = self._build_critic(learning_rate=self.critic_lr)\n",
    "\n",
    "        # For debugging\n",
    "        self.critic_Q.summary()\n",
    "\n",
    "        # Target networks\n",
    "        self.actor_hat = self._build_actor(learning_rate=self.actor_lr)\n",
    "        self.actor_hat.set_weights(self.actor.get_weights())\n",
    "\n",
    "        self.critic_Q_ex_hat, self.critic_Q_ex2_hat, self.critic_Q_hat = self._build_critic(learning_rate=self.critic_lr)\n",
    "        self.critic_Q_ex_hat.set_weights(self.critic_Q_ex.get_weights())\n",
    "        self.critic_Q_ex2_hat.set_weights(self.critic_Q_ex2.get_weights())\n",
    "\n",
    "        # Epsilon-greedy\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.epsilon_min = 0.1\n",
    "\n",
    "        # Replay buffer\n",
    "        buffer_size = 600000\n",
    "        prioritized_replay_alpha = 0.6\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        prioritized_replay_beta0 = 0.4\n",
    "        prioritized_replay_beta_iters = 50001\n",
    "        self.beta_schedule = LinearSchedule(\n",
    "            prioritized_replay_beta_iters,\n",
    "            initial_p=prioritized_replay_beta0,\n",
    "            final_p=1.0\n",
    "        )\n",
    "        self.prioritized_replay_eps = 1e-6\n",
    "\n",
    "        # memory sample batch size\n",
    "        self.batch_size = 128\n",
    "        self.t = None\n",
    "\n",
    "        # Optimizers\n",
    "        self.actor_optimizer = Adam(self.actor_lr)\n",
    "        self.critic_Q_ex_optimizer = Adam(self.critic_lr)\n",
    "        self.critic_Q_ex2_optimizer = Adam(self.critic_lr)\n",
    "        # The Q is a function of Q_ex and Q_ex2, so we won't train Q directly\n",
    "        # with compile, we do it via the Q_ex, Q_ex2 fits or custom step.\n",
    "        # But we keep a reference if you want to do a single-step approach.\n",
    "\n",
    "    def load(self, tag=\"\"):\n",
    "        \"\"\"Load weights from h5 files.\"\"\"\n",
    "        if tag == \"\":\n",
    "            actor_file = \"weights/bs/3month_daily/ddpg_actor.h5\"\n",
    "            critic_Q_ex_file = \"weights/bs/3month_daily/ddpg_critic_Q_ex.h5\"\n",
    "            critic_Q_ex2_file = \"weights/bs/3month_daily/ddpg_critic_Q_ex2.h5\"\n",
    "        else:\n",
    "            actor_file = f\"weights/bs/3month_daily/ddpg_actor_{tag}.h5\"\n",
    "            critic_Q_ex_file = f\"weights/bs/3month_daily/ddpg_critic_Q_ex_{tag}.h5\"\n",
    "            critic_Q_ex2_file = f\"weights/bs/3month_daily/ddpg_critic_Q_ex2_{tag}.h5\"\n",
    "\n",
    "        if os.path.exists(actor_file):\n",
    "            self.actor.load_weights(actor_file)\n",
    "            self.actor_hat.load_weights(actor_file)\n",
    "        if os.path.exists(critic_Q_ex_file):\n",
    "            self.critic_Q_ex.load_weights(critic_Q_ex_file)\n",
    "            self.critic_Q_ex_hat.load_weights(critic_Q_ex_file)\n",
    "        if os.path.exists(critic_Q_ex2_file):\n",
    "            self.critic_Q_ex2.load_weights(critic_Q_ex2_file)\n",
    "            self.critic_Q_ex2_hat.load_weights(critic_Q_ex2_file)\n",
    "\n",
    "    def _build_actor(self, learning_rate=1e-3):\n",
    "        \"\"\"Basic Actor NN.\"\"\"\n",
    "        inputs = Input(shape=(self.env.num_state,))\n",
    "        x = BatchNormalization()(inputs)\n",
    "\n",
    "        x = Dense(32, activation=\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        # Final output\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)\n",
    "        output = Lambda(lambda x: x * self.env.num_contract * 100)(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate))\n",
    "        return model\n",
    "\n",
    "    def _build_critic(self, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        We build 2 critics, Q_ex and Q_ex2, and a derived Q = Q_ex - ra_c * sqrt(max(Q_ex2 - Q_ex^2, 0)).\n",
    "        Note that Q_ex2 is the second moment, so Q_ex2 - Q_ex^2 is the variance, etc.\n",
    "        \"\"\"\n",
    "        s_inputs = Input(shape=(self.env.num_state,))\n",
    "        a_inputs = Input(shape=(1,))\n",
    "\n",
    "        # combine\n",
    "        x = concatenate([s_inputs, a_inputs])\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        # Q_ex\n",
    "        x1 = Dense(32, activation=\"relu\")(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = Dense(64, activation=\"relu\")(x1)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        output1 = Dense(1, activation=\"linear\")(x1)\n",
    "        model_Q_ex = Model(inputs=[s_inputs, a_inputs], outputs=output1)\n",
    "        model_Q_ex.compile(loss=\"mse\", optimizer=Adam(learning_rate))\n",
    "\n",
    "        # Q_ex2\n",
    "        x2 = Dense(32, activation=\"relu\")(x)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = Dense(64, activation=\"relu\")(x2)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        output2 = Dense(1, activation=\"linear\")(x2)\n",
    "        model_Q_ex2 = Model(inputs=[s_inputs, a_inputs], outputs=output2)\n",
    "        model_Q_ex2.compile(loss=\"mse\", optimizer=Adam(learning_rate))\n",
    "\n",
    "        # Risk-averse Q\n",
    "        def custom_q(o):\n",
    "            # o[0] = Q_ex, o[1] = Q_ex2\n",
    "            # We define: Q = Q_ex - ra_c * sqrt(max(Q_ex2 - Q_ex^2, 0))\n",
    "            # In Keras Lambda, be careful with TF ops\n",
    "            q_ex = o[0]\n",
    "            q_ex2 = o[1]\n",
    "            variance = q_ex2 - tf.square(q_ex)\n",
    "            clipped_variance = tf.maximum(variance, 0.0)\n",
    "            return q_ex - self.ra_c * tf.sqrt(clipped_variance)\n",
    "\n",
    "        output3 = Lambda(custom_q, output_shape=(1,))([output1, output2])\n",
    "        model_Q = Model(inputs=[s_inputs, a_inputs], outputs=output3)\n",
    "        # Typically, we won't train model_Q directly. If you do:\n",
    "        model_Q.compile(loss=\"mse\", optimizer=Adam(learning_rate))\n",
    "\n",
    "        return model_Q_ex, model_Q_ex2, model_Q\n",
    "\n",
    "    def egreedy_action(self, X):\n",
    "        \"\"\"\n",
    "        Epsilon-greedy approach. \n",
    "        We do not use OU noise here, but you could adapt it if needed.\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:  # or tf.random.uniform() if you prefer\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = self.actor.predict(X)[0]\n",
    "        return action, None, None\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Decay epsilon after each episode.\"\"\"\n",
    "        if self.epsilon >= self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in the replay buffer.\"\"\"\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def process_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample from replay buffer, prepare targets for Q_ex and Q_ex2,\n",
    "        and compute TD-error for priorities.\n",
    "        \"\"\"\n",
    "        experience = self.replay_buffer.sample(batch_size)\n",
    "        (states, actions, rewards, next_states, dones) = experience\n",
    "\n",
    "        actions = actions.reshape(-1, 1)\n",
    "        rewards = rewards.reshape(-1, 1)\n",
    "        dones = dones.reshape(-1, 1)\n",
    "\n",
    "        # next_actions from target actor\n",
    "        next_actions = self.actor_hat.predict(next_states)\n",
    "\n",
    "        # Q_ex_hat, Q_ex2_hat for next state\n",
    "        q_ex_next = self.critic_Q_ex_hat.predict([next_states, next_actions])\n",
    "        q_ex2_next = self.critic_Q_ex2_hat.predict([next_states, next_actions])\n",
    "\n",
    "        target_q_ex = rewards + (1 - dones) * q_ex_next\n",
    "        target_q_ex2 = rewards**2 + (1 - dones) * (2 * rewards * q_ex_next + q_ex2_next)\n",
    "\n",
    "        # Use Q_ex2 TD error as priority\n",
    "        #td_errors = self.critic_Q_ex2.predict([states, actions]) - target_q_ex2\n",
    "        #new_priorities = (np.abs(td_errors) + self.prioritized_replay_eps).flatten()\n",
    "        #self.replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
    "\n",
    "        return states, actions, target_q_ex, target_q_ex2,\n",
    "\n",
    "    def update_critics(self, states, actions, y1, y2):\n",
    "        \"\"\"\n",
    "        Update Q_ex and Q_ex2 via .fit().\n",
    "        \"\"\"\n",
    "        # Flatten if needed\n",
    "        #weights = weights.flatten()\n",
    "\n",
    "        # Q_ex\n",
    "        hist_ex = self.critic_Q_ex.fit(\n",
    "            [states, actions],\n",
    "            y1,\n",
    "            verbose=0\n",
    "        )\n",
    "        loss_ex = np.mean(hist_ex.history['loss'])\n",
    "\n",
    "        # Q_ex2\n",
    "        hist_ex2 = self.critic_Q_ex2.fit(\n",
    "            [states, actions],\n",
    "            y2,\n",
    "            verbose=0\n",
    "        )\n",
    "        loss_ex2 = np.mean(hist_ex2.history['loss'])\n",
    "\n",
    "        return loss_ex, loss_ex2\n",
    "\n",
    "    @tf.function\n",
    "    def update_actor_tf(self, states):\n",
    "        \"\"\"\n",
    "        Update the actor using tf.GradientTape.\n",
    "        In DDPG, we want to maximize Q(s, a), so we minimize -Q(s, a).\n",
    "        We'll use self.critic_Q for that.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute actions from current actor\n",
    "            actions = self.actor(states, training=True)\n",
    "            # Evaluate Q from critic_Q\n",
    "            q_values = self.critic_Q([states, actions], training=False)\n",
    "            # We want to maximize q_values -> minimize -q_values\n",
    "            actor_loss = -tf.reduce_mean(q_values)\n",
    "\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        return actor_loss\n",
    "\n",
    "    def update_model(self, X1, X2, y1, y2):\n",
    "        \"\"\"\n",
    "        1) Update critics (Q_ex, Q_ex2) using supervised fit.\n",
    "        2) Update actor using policy gradient wrt the Q.\n",
    "        \"\"\"\n",
    "        loss_ex, loss_ex2 = self.update_critics(X1, X2, y1, y2)\n",
    "\n",
    "        # Now update the actor\n",
    "        # In TF2, we can directly do: self.update_actor_tf(...)\n",
    "        # Convert X1 to tf.Tensor if it isn't already\n",
    "        states_tf = tf.convert_to_tensor(X1, dtype=tf.float32)\n",
    "        actor_loss = self.update_actor_tf(states_tf)\n",
    "\n",
    "        return loss_ex, loss_ex2, actor_loss\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Soft update target networks.\"\"\"\n",
    "        critic_Q_ex_weights = self.critic_Q_ex.get_weights()\n",
    "        critic_Q_ex2_weights = self.critic_Q_ex2.get_weights()\n",
    "        actor_weights = self.actor.get_weights()\n",
    "\n",
    "        critic_Q_ex_hat_weights = self.critic_Q_ex_hat.get_weights()\n",
    "        critic_Q_ex2_hat_weights = self.critic_Q_ex2_hat.get_weights()\n",
    "        actor_hat_weights = self.actor_hat.get_weights()\n",
    "\n",
    "        \"\"\"for i in range(len(critic_Q_ex_weights)):\n",
    "            critic_Q_ex_hat_weights[i] = (\n",
    "                self.TAU * critic_Q_ex_weights[i] +\n",
    "                (1 - self.TAU) * critic_Q_ex_hat_weights[i]\n",
    "            )\n",
    "\n",
    "        for i in range(len(critic_Q_ex2_weights)):\n",
    "            critic_Q_ex2_hat_weights[i] = (\n",
    "                self.TAU * critic_Q_ex2_weights[i] +\n",
    "                (1 - self.TAU) * critic_Q_ex2_hat_weights[i]\n",
    "            )\"\"\"\n",
    "\n",
    "        for i in range(len(actor_weights)):\n",
    "            actor_hat_weights[i] = (\n",
    "                self.TAU * actor_weights[i] +\n",
    "                (1 - self.TAU) * actor_hat_weights[i]\n",
    "            )\n",
    "\n",
    "        self.critic_Q_ex_hat.set_weights(critic_Q_ex_hat_weights)\n",
    "        self.critic_Q_ex2_hat.set_weights(critic_Q_ex2_hat_weights)\n",
    "        self.actor_hat.set_weights(actor_hat_weights)\n",
    "\n",
    "    def train(self, episode):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "        \"\"\"\n",
    "        history = {\"episode\": [], \"episode_w_T\": [], \"loss_ex\": [], \"loss_ex2\": []}\n",
    "\n",
    "        for i in range(episode):\n",
    "            observation = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            y_action = []\n",
    "            reward_store = []\n",
    "\n",
    "            self.t = i  # for beta-schedule in replay buffer\n",
    "\n",
    "            while not done:\n",
    "                try:\n",
    "                    x = np.array(observation).reshape(1, -1)\n",
    "                except:\n",
    "                    observation = [observation[0],observation[1][0], observation[2]]\n",
    "                    x = np.array(observation).reshape(1, -1)\n",
    "                action, _, _ = self.egreedy_action(x)\n",
    "                observation, reward, done, info = self.env.step(action)\n",
    "                y_action.append(action[0])\n",
    "                reward_store.append(reward[0])\n",
    "\n",
    "                # remember\n",
    "                self.remember(x[0], action, reward, observation, done)\n",
    "\n",
    "                # If buffer has enough data, start training\n",
    "                if len(self.replay_buffer) > self.batch_size:\n",
    "                    X1, X2, y_ex, y_ex2= self.process_batch(self.batch_size)\n",
    "                    loss_ex, loss_ex2, actor_loss = self.update_model(X1, X2, y_ex, y_ex2)\n",
    "                    self.update_target_model()\n",
    "\n",
    "            # Epsilon decay at the end of each episode\n",
    "            self.update_epsilon()\n",
    "\n",
    "            # Print/store stats every 1000 episodes\n",
    "            if i % 50 == 0 and i != 0:\n",
    "                w_T = np.sum(reward_store)\n",
    "                history[\"episode\"].append(i)\n",
    "                history[\"episode_w_T\"].append(w_T)\n",
    "                history[\"loss_ex\"].append(loss_ex)\n",
    "                history[\"loss_ex2\"].append(loss_ex2)\n",
    "\n",
    "                path_row = info[\"path_row\"]  # if your env provides this\n",
    "                print(info)\n",
    "                print(\n",
    "                    \"episode: {} | final wealth: {:.3f} | loss_ex: {:.3f} | loss_ex2: {:.3f} | epsilon:{:.2f}\".format(\n",
    "                        i, w_T, loss_ex, loss_ex2, self.epsilon\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "                with np.printoptions(precision=2, suppress=True):\n",
    "                    print(\"episode: {} | rewards {}\".format(i, reward_store))\n",
    "                    print(\"episode: {} | actions taken {}\".format(i, y_action))\n",
    "                    print(\"episode: {} | deltas {}\".format(i, self.env.delta_path[path_row] * 100))\n",
    "                    print(\"episode: {} | stock price {}\".format(i, self.env.path[path_row]))\n",
    "                    print(\"episode: {} | option price {}\\n\".format(i, self.env.option_price_path[path_row] * 100))\n",
    "            if i % 100 == 0 and i != 0:\n",
    "                # Saving model\n",
    "                self.actor.save_weights(\"model/ddpg_actor_\" + str(int(i / 100))+'.weights' + \".h5\")\n",
    "                self.critic_Q_ex.save_weights(\"model/ddpg_critic_Q_ex_\" + str(int(i / 100))+'.weights' + \".h5\")\n",
    "                self.critic_Q_ex2.save_weights(\"model/ddpg_critic_Q_ex2_\" + str(int(i / 100)) +'.weights' \".h5\")\n",
    "\n",
    "        # Final save\n",
    "        self.actor.save_weights(\"model/ddpg_actor.weights.h5\")\n",
    "        self.critic_Q_ex.save_weights(\"model/ddpg_critic_Q_ex.weights.h5\")\n",
    "        self.critic_Q_ex2.save_weights(\"model/ddpg_critic_Q_ex2.weights.h5\")\n",
    "\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. generate asset price paths\n",
      "2. generate BS price and delta\n",
      "simulation done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_67\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_67\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_49      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_50      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_16      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_layer_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ concatenate_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_147 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_150 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dense_147[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dense_150[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_148 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_151 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_148[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_151[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_149 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_152 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_149[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ dense_152[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_49      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_50      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_16      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ input_layer_49[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_layer_50[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │         \u001b[38;5;34m16\u001b[0m │ concatenate_16[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_147 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m160\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_150 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m160\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m128\u001b[0m │ dense_147[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m128\u001b[0m │ dense_150[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_148 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m2,112\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_151 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m2,112\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m256\u001b[0m │ dense_148[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m256\u001b[0m │ dense_151[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_149 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_152 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_33 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_149[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ dense_152[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,458</span> (21.32 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,458\u001b[0m (21.32 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,066</span> (19.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,066\u001b[0m (19.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">392</span> (1.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m392\u001b[0m (1.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "simulation =100\n",
    "env_test = TradingEnv(spread=0.0, num_contract=1, init_ttm=20, trade_freq=1, num_sim=simulation)\n",
    "ddpg_test = DDPG(env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***\n",
      "Testing agent actions.\n",
      "Testing model saved at 6K episode.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n***\")\n",
    "tag=\"6\"\n",
    "delta_action_test = False\n",
    "if delta_action_test:\n",
    "    print(\"Testing delta actions.\")\n",
    "else:\n",
    "    print(\"Testing agent actions.\")\n",
    "    if tag == \"\":\n",
    "        print(\"tesing the model saved at the end of the training.\")\n",
    "    else:\n",
    "        print(\"Testing model saved at \" + tag + \"K episode.\")\n",
    "    actor = ddpg_test.load(tag=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mddpg_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 325\u001b[0m, in \u001b[0;36mDDPG.train\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# If buffer has enough data, start training\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m--> 325\u001b[0m     X1, X2, y_ex, y_ex2\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     loss_ex, loss_ex2, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_model(X1, X2, y_ex, y_ex2)\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target_model()\n",
      "Cell \u001b[0;32mIn[54], line 179\u001b[0m, in \u001b[0;36mDDPG.process_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[1;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    Sample from replay buffer, prepare targets for Q_ex and Q_ex2,\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    and compute TD-error for priorities.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     experience \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     (states, actions, rewards, next_states, dones) \u001b[38;5;241m=\u001b[39m experience\n\u001b[1;32m    182\u001b[0m     actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 62\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sample a batch of experiences.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    the end of an episode and 0 otherwise.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m idxes \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)]\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43midxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 35\u001b[0m, in \u001b[0;36mReplayBuffer._encode_sample\u001b[0;34m(self, idxes)\u001b[0m\n\u001b[1;32m     33\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(action, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     34\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m---> 35\u001b[0m     obses_tp1\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     36\u001b[0m     dones\u001b[38;5;241m.\u001b[39mappend(done)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(obses_t), np\u001b[38;5;241m.\u001b[39marray(actions), np\u001b[38;5;241m.\u001b[39marray(rewards), np\u001b[38;5;241m.\u001b[39marray(obses_tp1), np\u001b[38;5;241m.\u001b[39marray(dones)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "ddpg_test.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
